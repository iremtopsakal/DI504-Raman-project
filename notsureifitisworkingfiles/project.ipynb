{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f550ff1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f59e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from resnet import ResNet\n",
    "from training import run_epoch\n",
    "import matplotlib.pyplot as plt\n",
    "from data import RamanSpectraDataset  \n",
    "# from data2 import RamanSpectraDataset  \n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.trial import Trial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from augment import apply_augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17eea56",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332cb396",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        # Layers\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=5,\n",
    "            stride=stride, padding=2, dilation=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=5,\n",
    "            stride=1, padding=2, dilation=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1,\n",
    "                    stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, hidden_sizes, num_blocks, input_dim=1000,\n",
    "        in_channels=64, n_classes=1):\n",
    "        super(ResNet, self).__init__()\n",
    "        assert len(num_blocks) == len(hidden_sizes)\n",
    "        self.input_dim = input_dim\n",
    "        self.in_channels = in_channels\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(1, self.in_channels, kernel_size=5, stride=1,\n",
    "            padding=2, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(self.in_channels)\n",
    "        \n",
    "        # Flexible number of residual encoding layers\n",
    "        layers = []\n",
    "        strides = [1] + [2] * (len(hidden_sizes) - 1)\n",
    "        for idx, hidden_size in enumerate(hidden_sizes):\n",
    "            layers.append(self._make_layer(hidden_size, num_blocks[idx],\n",
    "                stride=strides[idx]))\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "        self.z_dim = self._get_encoding_size()\n",
    "        self.linear = nn.Linear(self.z_dim, self.n_classes)\n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.encoder(x)\n",
    "        z = x.view(x.size(0), -1)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        return self.linear(z)\n",
    "\n",
    "\n",
    "    def _make_layer(self, out_channels, num_blocks, stride=1):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        blocks = []\n",
    "        for stride in strides:\n",
    "            blocks.append(ResidualBlock(self.in_channels, out_channels,\n",
    "                stride=stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*blocks)\n",
    "\n",
    "    def _get_encoding_size(self):\n",
    "        \"\"\"\n",
    "        Returns the dimension of the encoded input.\n",
    "        \"\"\"\n",
    "        temp = Variable(torch.rand(1, 1, self.input_dim))\n",
    "        z = self.encode(temp)\n",
    "        z_dim = z.data.size(1)\n",
    "        return z_dim\n",
    "\n",
    "\n",
    "def add_activation(activation='relu'):\n",
    "    \"\"\"\n",
    "    Adds specified activation layer, choices include:\n",
    "    - 'relu'\n",
    "    - 'elu' (alpha)\n",
    "    - 'selu'\n",
    "    - 'leaky relu' (negative_slope)\n",
    "    - 'sigmoid'\n",
    "    - 'tanh'\n",
    "    - 'softplus' (beta, threshold)\n",
    "    \"\"\"\n",
    "    if activation == 'relu':\n",
    "        return nn.ReLU()\n",
    "    elif activation == 'elu':\n",
    "        return nn.ELU(alpha=1.0)\n",
    "    elif activation == 'selu':\n",
    "        return nn.SELU()\n",
    "    elif activation == 'leaky relu':\n",
    "        return nn.LeakyReLU(negative_slope=0.1)\n",
    "    elif activation == 'sigmoid':\n",
    "        return nn.Sigmoid()\n",
    "    elif activation == 'tanh':\n",
    "        return nn.Tanh()\n",
    "    # SOFTPLUS DOESN'T WORK with automatic differentiation in pytorch\n",
    "    elif activation == 'softplus':\n",
    "        return nn.Softplus(beta=1, threshold=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b3f01d",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfa73ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Dataset Definition ===\n",
    "class RamanSpectraDataset(Dataset):\n",
    "    def __init__(self, root_dir, augment=False, offline_aug=False, num_aug=2):\n",
    "        self.samples = []\n",
    "        self.augment = augment\n",
    "        self.offline_aug = offline_aug\n",
    "        self.num_aug = num_aug\n",
    "\n",
    "        for folder in os.listdir(root_dir):\n",
    "            folder_path = os.path.join(root_dir, folder)\n",
    "            if not os.path.isdir(folder_path):\n",
    "                continue\n",
    "            try:\n",
    "                conc = float(f\"1e-{folder}\")\n",
    "                label = np.log10(conc)  # e.g., -5.0, -6.0\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            #for fname in os.listdir(folder_path):\n",
    "                if fname.endswith('.txt'):\n",
    "                    fpath = os.path.join(folder_path, fname)\n",
    "                    self.samples.append((fpath, label))\n",
    "            \n",
    "            for fname in os.listdir(folder_path):\n",
    "                if fname.endswith('.txt'):\n",
    "                    fpath = os.path.join(folder_path, fname)\n",
    "                    self.samples.append((fpath, label))\n",
    "\n",
    "                if self.offline_aug:\n",
    "                    for i in range(self.num_aug):\n",
    "                        self.samples.append((fpath, label, True))  # Third item marks \"needs augmentation\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.samples[idx]\n",
    "\n",
    "        if len(entry) == 3:\n",
    "            fpath, label, to_augment = entry\n",
    "        else:\n",
    "            fpath, label = entry\n",
    "            to_augment = False\n",
    "\n",
    "        data = np.loadtxt(fpath)\n",
    "        intensities = data[:, 1]\n",
    "\n",
    "        if self.augment or to_augment:\n",
    "            intensities = apply_augmentation(intensities)\n",
    "\n",
    "        # Normalize\n",
    "        intensities = (intensities - intensities.mean()) / (intensities.std() + 1e-8)\n",
    "        x = torch.tensor(intensities[np.newaxis, :], dtype=torch.float32)\n",
    "        y = torch.tensor(label, dtype=torch.float32)\n",
    "        return x, y\n",
    "    \n",
    "    #def __getitem__(self, idx):\n",
    "        fpath, label = self.samples[idx]\n",
    "        data = np.loadtxt(fpath)\n",
    "        intensities = data[:, 1]\n",
    "\n",
    "        if self.augment:\n",
    "            intensities = apply_augmentation(intensities)\n",
    "\n",
    "        intensities = (intensities - intensities.mean()) / (intensities.std() + 1e-8)\n",
    "        x = torch.tensor(intensities[np.newaxis, :], dtype=torch.float32)\n",
    "        y = torch.tensor(label, dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #def __getitem__(self, idx):\n",
    "        fpath, label = self.samples[idx]\n",
    "        data = np.loadtxt(fpath)\n",
    "        intensities = data[:, 1]\n",
    "        intensities = (intensities - intensities.mean()) / (intensities.std() + 1e-8)\n",
    "        x = torch.tensor(intensities[np.newaxis, :], dtype=torch.float32)\n",
    "        y = torch.tensor(label, dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "# === Load & Split Dataset ===\n",
    "def get_loaders(root_path, batch_size=32):\n",
    "    dataset = RamanSpectraDataset(root_path)\n",
    "\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size   = int(0.1 * len(dataset))\n",
    "    test_size  = len(dataset) - train_size - val_size\n",
    "\n",
    "    train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_set, batch_size=batch_size)\n",
    "    test_loader  = DataLoader(test_set, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a068c7",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ab5e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(epoch, model, dataloader, cuda, training=False, optimizer=None):\n",
    "    if training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch} {'Train' if training else 'Val'}\")):\n",
    "        if cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = Variable(inputs), Variable(targets.float())\n",
    "\n",
    "        outputs = model(inputs).squeeze()\n",
    "        loss = nn.MSELoss()(outputs, targets)\n",
    "\n",
    "        if training:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * targets.size(0)\n",
    "        total_samples += targets.size(0)\n",
    "\n",
    "        all_preds.extend(outputs.detach().cpu().numpy())\n",
    "        all_targets.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "\n",
    "    # === Optional: Round predictions to nearest integer log10 value ===\n",
    "    # You can comment or delete this block if it hurts performance\n",
    "    all_preds = [min(-5, max(-9, int(round(pred)))) for pred in all_preds]\n",
    "\n",
    "    mae = mean_absolute_error(all_targets, all_preds)\n",
    "    rmse = root_mean_squared_error(all_targets, all_preds)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "    return avg_loss, mae, rmse, r2\n",
    "\n",
    "\n",
    "def get_predictions(model, dataloader, cuda, get_probs=False):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        if cuda: inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = Variable(inputs), Variable(targets.float())\n",
    "        outputs = model(inputs)\n",
    "        if get_probs:\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            if cuda: probs = probs.data.cpu().numpy()\n",
    "            else: probs = probs.data.numpy()\n",
    "            preds.append(probs)\n",
    "        else:\n",
    "            predicted = outputs.squeeze()\n",
    "            preds += list(predicted.detach().cpu().numpy())\n",
    "    if get_probs:\n",
    "        return np.vstack(preds)\n",
    "    else:\n",
    "        return np.array(preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
