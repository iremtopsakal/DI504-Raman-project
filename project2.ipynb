{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d089054",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93f243da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Irem\\Documents\\DI504\\Project\\Python\\venv310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from resnet import ResNet\n",
    "from training import run_epoch\n",
    "import matplotlib.pyplot as plt\n",
    "from data import RamanSpectraDataset  \n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.trial import Trial\n",
    "from augment import AugmentedWrapper\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc64e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97480491",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7bb8ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dataset size (before split): 4500\n",
      "Input dimension: 2030\n"
     ]
    }
   ],
   "source": [
    "data_path = \"Data/ALS baseline corrected normalized merged\" # Change based on your dataset location\n",
    "\n",
    "dataset = RamanSpectraDataset(\n",
    "    data_path,\n",
    "    augment=False,       \n",
    "    offline_aug=False     \n",
    ")\n",
    "sample_x, _ = dataset[0]\n",
    "input_dim = sample_x.shape[-1]\n",
    "\n",
    "print(f\"Raw dataset size (before split): {len(dataset)}\")\n",
    "print(f\"Input dimension: {input_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42fd20b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples (only raw): 3150\n",
      "Training samples (with augentation added): 9450\n",
      "Validation samples: 675\n",
      "Test samples: 675\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "print(f\"Training samples (only raw): {len(train_set)}\")\n",
    "\n",
    "train_set = AugmentedWrapper(train_set, num_aug=2)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32)\n",
    "test_loader = DataLoader(test_set, batch_size=32)\n",
    "\n",
    "print(f\"Training samples (with augentation added): {len(train_set)}\")\n",
    "print(f\"Validation samples: {len(val_set)}\")\n",
    "print(f\"Test samples: {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6879c3f0",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7bee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-20 19:51:15,749] A new study created in memory with name: no-name-79d760b0-202c-4fd0-b037-7070b090b3ba\n",
      "Epoch 1 Train:  13%|█▎        | 39/296 [00:03<00:18, 13.65it/s]"
     ]
    }
   ],
   "source": [
    "def objective(trial: Trial):\n",
    "    hidden_sizes = [\n",
    "        trial.suggest_categorical(\"hidden_1\", [32, 64, 128]),\n",
    "        trial.suggest_categorical(\"hidden_2\", [64, 128, 256]),\n",
    "        trial.suggest_categorical(\"hidden_3\", [128, 256, 512])\n",
    "    ]\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "\n",
    "    model = ResNet(\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        num_blocks=[2, 2, 2],\n",
    "        input_dim=input_dim,\n",
    "        n_classes=1\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Short training for tuning\n",
    "    for epoch in range(1, 6):\n",
    "        train_loss, *_ = run_epoch(epoch, model, train_loader, cuda=torch.cuda.is_available(),\n",
    "                                   training=True, optimizer=optimizer)\n",
    "        val_loss, *_ = run_epoch(epoch, model, val_loader, cuda=torch.cuda.is_available(),\n",
    "                                 training=False)\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "print(\"Best params:\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1bc55f",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c97d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hidden = [\n",
    "    study.best_params[\"hidden_1\"],\n",
    "    study.best_params[\"hidden_2\"],\n",
    "    study.best_params[\"hidden_3\"]\n",
    "]\n",
    "best_lr = study.best_params[\"lr\"]\n",
    "\n",
    "model = ResNet(\n",
    "    hidden_sizes=best_hidden,\n",
    "    num_blocks=[2, 2, 2],\n",
    "    input_dim=input_dim,\n",
    "    n_classes=1\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69f3206",
   "metadata": {},
   "source": [
    "## Define metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802eb908",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses = [], []\n",
    "train_maes, val_maes = [], []\n",
    "train_rmses, val_rmses = [], []\n",
    "train_r2s, val_r2s = [], []\n",
    "train_loss_iters, val_loss_iters = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d531cf",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467c1cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 21):\n",
    "    train_loss, train_mae, train_rmse, train_r2, train_kappa, train_conf, train_batch_losses = run_epoch(\n",
    "        epoch, model, train_loader, cuda=torch.cuda.is_available(), training=True, optimizer=optimizer\n",
    "    )\n",
    "    val_loss, val_mae, val_rmse, val_r2, val_kappa, val_conf, val_batch_losses = run_epoch(\n",
    "        epoch, model, val_loader, cuda=torch.cuda.is_available(), training=False\n",
    "    )\n",
    "\n",
    "    train_loss_iters.extend(train_batch_losses)\n",
    "    val_loss_iters.extend(val_batch_losses)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_maes.append(train_mae)\n",
    "    val_maes.append(val_mae)\n",
    "    train_rmses.append(train_rmse)\n",
    "    val_rmses.append(val_rmse)\n",
    "    train_r2s.append(train_r2)\n",
    "    val_r2s.append(val_r2)\n",
    "\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    print(f\"  Train: Loss={train_loss:.4f}, MAE={train_mae:.4f}, RMSE={train_rmse:.4f}, R2={train_r2:.4f}, Kappa={train_kappa:.4f}\")\n",
    "    print(f\"  Val  : Loss={val_loss:.4f}, MAE={val_mae:.4f}, RMSE={val_rmse:.4f}, R2={val_r2:.4f}, Kappa={val_kappa:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401656d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"cv_model.ckpt\")\n",
    "print(\"Model saved as cv_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f6a09e",
   "metadata": {},
   "source": [
    "## Performance evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50d7565",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, 21)\n",
    "\n",
    "def plot_loss_graph(loss_list):\n",
    "    \"\"\"Plot smoothed training and validation loss curves using moving average.\n",
    "    Validation loss is interpolated to match the number of training iterations.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    train_loss = loss_list[0]\n",
    "    val_loss = loss_list[1]\n",
    "\n",
    "    if len(train_loss) > len(val_loss):\n",
    "        val_loss_interp = np.interp(\n",
    "            np.linspace(0, len(val_loss) - 1, len(train_loss)),\n",
    "            np.arange(len(val_loss)),\n",
    "            val_loss\n",
    "        )\n",
    "    else:\n",
    "        val_loss_interp = val_loss\n",
    "\n",
    "    filter_size = max(1, len(train_loss) // 10)\n",
    "    kernel = np.ones(filter_size) / filter_size\n",
    "    train_smoothed = np.convolve(train_loss, kernel, mode='valid')\n",
    "    val_smoothed = np.convolve(val_loss_interp, kernel, mode='valid')\n",
    "\n",
    "    plt.plot(train_smoothed, label='Train Loss')\n",
    "    plt.plot(val_smoothed, label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.title(\"Smoothed Loss Curves\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"loss.png\")\n",
    "\n",
    "plot_loss_graph([train_loss_iters, val_loss_iters])\n",
    "plt.savefig(\"loss_plot_smoothed.png\")\n",
    "\n",
    "def plot_metric(train_vals, val_vals, ylabel, title, filename):\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_vals, label='Train')\n",
    "    plt.plot(epochs, val_vals, label='Validation')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(filename)\n",
    "\n",
    "plot_metric(train_losses, val_losses, \"Loss\", \"Loss over Epochs\", \"loss_plot.png\")\n",
    "plot_metric(train_maes, val_maes, \"MAE\", \"Mean Absolute Error over Epochs\", \"mae_plot.png\")\n",
    "plot_metric(train_rmses, val_rmses, \"RMSE\", \"Root Mean Squared Error over Epochs\", \"rmse_plot.png\")\n",
    "plot_metric(train_r2s, val_r2s, \"R²\", \"R² Score over Epochs\", \"r2_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eed0949",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_mae, test_rmse, test_r2, test_kappa, test_conf, test_batch_losses = run_epoch(\n",
    "    \"Test\", model, test_loader, cuda=torch.cuda.is_available(), training=False\n",
    ")\n",
    "\n",
    "test_confusion_df = pd.DataFrame(test_conf, index=[-5, -6, -7, -8, -9], columns=[-5, -6, -7, -8, -9])\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(test_confusion_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Test Confusion Matrix\")\n",
    "plt.savefig(\"confusion_matrix_test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17be88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "test_preds = []\n",
    "test_actuals = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        outputs = model(x).squeeze()\n",
    "        preds = [min(-5, max(-9, int(round(p.item())))) for p in outputs]\n",
    "        labels = [int(round(t.item())) for t in y]\n",
    "\n",
    "        test_preds.extend(preds)\n",
    "        test_actuals.extend(labels)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "target_names = ['1e-5', '1e-6', '1e-7', '1e-8', '1e-9']\n",
    "print(classification_report(test_actuals, test_preds, labels=[-5, -6, -7, -8, -9], target_names=target_names))\n",
    "\n",
    "test_acc = accuracy_score(test_actuals, test_preds)\n",
    "print(f\"Test Accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d61fd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total trainable parameters: {count_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b493654d",
   "metadata": {},
   "source": [
    "## Baseline Comparison: Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aba0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinearRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleLinearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.linear(x)\n",
    "\n",
    "baseline_model = SimpleLinearRegression(input_dim).to(device)\n",
    "baseline_optimizer = torch.optim.Adam(baseline_model.parameters(), lr=1e-3)\n",
    "\n",
    "baseline_train_losses, baseline_val_losses = [], []\n",
    "baseline_train_batch_losses = []\n",
    "baseline_val_batch_losses = []\n",
    "\n",
    "for epoch in range(1, 21):\n",
    "    train_loss, _, _, _, _, _, train_batches = run_epoch(\n",
    "        epoch, baseline_model, train_loader, cuda=torch.cuda.is_available(),\n",
    "        training=True, optimizer=baseline_optimizer\n",
    "    )\n",
    "    val_loss, _, _, _, _, _, val_batches = run_epoch(\n",
    "        epoch, baseline_model, val_loader, cuda=torch.cuda.is_available(),\n",
    "        training=False\n",
    "    )\n",
    "    baseline_train_losses.append(train_loss)\n",
    "    baseline_val_losses.append(val_loss)\n",
    "    baseline_train_batch_losses.extend(train_batches)\n",
    "    baseline_val_batch_losses.extend(val_batches)\n",
    "\n",
    "\n",
    "test_loss, test_mae, test_rmse, test_r2, test_kappa, test_conf, _ = run_epoch(\n",
    "    \"Test-Baseline\", baseline_model, test_loader, cuda=torch.cuda.is_available(), training=False)\n",
    "\n",
    "print(\"\\nBaseline Linear Regression Test Results:\")\n",
    "print(f\"  Loss={test_loss:.4f}, MAE={test_mae:.4f}, RMSE={test_rmse:.4f}, R2={test_r2:.4f}, Kappa={test_kappa:.4f}\")\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "mlp_preds = []\n",
    "mlp_actuals = []\n",
    "\n",
    "baseline_model.eval()\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        outputs = baseline_model(x).squeeze()\n",
    "        preds = [min(-5, max(-9, int(round(p.item())))) for p in outputs]\n",
    "        labels = [int(round(t.item())) for t in y]\n",
    "\n",
    "        mlp_preds.extend(preds)\n",
    "        mlp_actuals.extend(labels)\n",
    "\n",
    "\n",
    "\n",
    "mlp_confusion_df = pd.DataFrame(test_conf, index=[-5, -6, -7, -8, -9], columns=[-5, -6, -7, -8, -9])\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(mlp_confusion_df, annot=True, fmt=\"d\", cmap=\"Purples\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Baseline Linear Regression Confusion Matrix\")\n",
    "plt.savefig(\"confusion_matrix_linear.png\")\n",
    "\n",
    "print(\"\\nBaseline Linear Regression Classification Report:\")\n",
    "print(classification_report(mlp_actuals, mlp_preds, labels=[-5, -6, -7, -8, -9], target_names=target_names))\n",
    "\n",
    "mlp_acc = accuracy_score(mlp_actuals, mlp_preds)\n",
    "print(f\"Baseline Linear Regression Test Accuracy: {mlp_acc:.3f}\")\n",
    "\n",
    "def plot_loss_graph_baseline(loss_list):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    train_loss = loss_list[0]\n",
    "    val_loss = loss_list[1]\n",
    "\n",
    "    if len(train_loss) > len(val_loss):\n",
    "        val_loss_interp = np.interp(\n",
    "            np.linspace(0, len(val_loss) - 1, len(train_loss)),\n",
    "            np.arange(len(val_loss)),\n",
    "            val_loss\n",
    "        )\n",
    "    else:\n",
    "        val_loss_interp = val_loss\n",
    "\n",
    "    filter_size = max(1, len(train_loss) // 10)\n",
    "    kernel = np.ones(filter_size) / filter_size\n",
    "    train_smoothed = np.convolve(train_loss, kernel, mode='valid')\n",
    "    val_smoothed = np.convolve(val_loss_interp, kernel, mode='valid')\n",
    "\n",
    "    plt.plot(train_smoothed, label='Train Loss')\n",
    "    plt.plot(val_smoothed, label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.title(\"Smoothed Loss Curves (Linear Regression)\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"loss_plot_linear.png\")\n",
    "\n",
    "plot_loss_graph_baseline([baseline_train_batch_losses, baseline_val_batch_losses])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9e3bb8",
   "metadata": {},
   "source": [
    "# Ablation Study: No Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8162a973",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = RamanSpectraDataset(\n",
    "    data_path,\n",
    "    augment=False,\n",
    "    offline_aug=False\n",
    ")\n",
    "train_raw, val_raw, test_raw = random_split(raw_dataset, [train_size, val_size, test_size])\n",
    "train_raw_loader = DataLoader(train_raw, batch_size=32, shuffle=True)\n",
    "val_raw_loader = DataLoader(val_raw, batch_size=32)\n",
    "test_raw_loader = DataLoader(test_raw, batch_size=32)\n",
    "\n",
    "ablation_model = ResNet(\n",
    "    hidden_sizes=best_hidden,\n",
    "    num_blocks=[2, 2, 2],\n",
    "    input_dim=input_dim,\n",
    "    n_classes=1\n",
    ").to(device)\n",
    "\n",
    "ablation_optimizer = torch.optim.Adam(ablation_model.parameters(), lr=best_lr)\n",
    "\n",
    "ablation_train_losses, ablation_val_losses = [], []\n",
    "ablation_train_batch_losses = []\n",
    "ablation_val_batch_losses = []\n",
    "\n",
    "for epoch in range(1, 21):\n",
    "    train_loss, _, _, _, _, _, train_batches = run_epoch(epoch, ablation_model, train_raw_loader, cuda=torch.cuda.is_available(), training=True, optimizer=ablation_optimizer)\n",
    "    val_loss, _, _, _, _, _, val_batches = run_epoch(epoch, ablation_model, val_raw_loader, cuda=torch.cuda.is_available(), training=False)\n",
    "\n",
    "    ablation_train_losses.append(train_loss)\n",
    "    ablation_val_losses.append(val_loss)\n",
    "    ablation_train_batch_losses.extend(train_batches)\n",
    "    ablation_val_batch_losses.extend(val_batches)\n",
    "\n",
    "test_loss, test_mae, test_rmse, test_r2, test_kappa, test_conf, _ = run_epoch(\n",
    "    \"Test-Ablation\", ablation_model, test_raw_loader, cuda=torch.cuda.is_available(), training=False\n",
    ")\n",
    "\n",
    "print(\"\\nAblation (No Augmentation) Test Results:\")\n",
    "print(f\"  Loss={test_loss:.4f}, MAE={test_mae:.4f}, RMSE={test_rmse:.4f}, R2={test_r2:.4f}, Kappa={test_kappa:.4f}\")\n",
    "\n",
    "ablation_preds = []\n",
    "ablation_actuals = []\n",
    "\n",
    "ablation_model.eval()\n",
    "with torch.no_grad():\n",
    "    for x, y in test_raw_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        outputs = ablation_model(x).squeeze()\n",
    "        preds = [min(-5, max(-9, int(round(p.item())))) for p in outputs]\n",
    "        labels = [int(round(t.item())) for t in y]\n",
    "\n",
    "        ablation_preds.extend(preds)\n",
    "        ablation_actuals.extend(labels)\n",
    "\n",
    "\n",
    "ablation_confusion_df = pd.DataFrame(test_conf, index=[-5, -6, -7, -8, -9], columns=[-5, -6, -7, -8, -9])\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(ablation_confusion_df, annot=True, fmt=\"d\", cmap=\"Oranges\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Ablation (No Augmentation) Confusion Matrix\")\n",
    "plt.savefig(\"confusion_matrix_ablation.png\")\n",
    "\n",
    "print(\"\\nAblation Model Classification Report:\")\n",
    "print(classification_report(ablation_actuals, ablation_preds, labels=[-5, -6, -7, -8, -9], target_names=target_names))\n",
    "\n",
    "ablation_acc = accuracy_score(ablation_actuals, ablation_preds)\n",
    "print(f\"Ablation Test Accuracy: {ablation_acc:.3f}\")\n",
    "\n",
    "def plot_loss_graph_ablation(loss_list):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    train_loss = loss_list[0]\n",
    "    val_loss = loss_list[1]\n",
    "\n",
    "    if len(train_loss) > len(val_loss):\n",
    "        val_loss_interp = np.interp(\n",
    "            np.linspace(0, len(val_loss) - 1, len(train_loss)),\n",
    "            np.arange(len(val_loss)),\n",
    "            val_loss\n",
    "        )\n",
    "    else:\n",
    "        val_loss_interp = val_loss\n",
    "\n",
    "    filter_size = max(1, len(train_loss) // 10)\n",
    "    kernel = np.ones(filter_size) / filter_size\n",
    "    train_smoothed = np.convolve(train_loss, kernel, mode='valid')\n",
    "    val_smoothed = np.convolve(val_loss_interp, kernel, mode='valid')\n",
    "\n",
    "    plt.plot(train_smoothed, label='Train Loss')\n",
    "    plt.plot(val_smoothed, label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.title(\"Smoothed Loss Curves (No Augmentation)\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"loss_plot_ablation.png\")\n",
    "\n",
    "plot_loss_graph_ablation([ablation_train_batch_losses, ablation_val_batch_losses])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
